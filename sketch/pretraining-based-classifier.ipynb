{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6053044a",
   "metadata": {},
   "source": [
    "## Train the pretraining+classifier model\n",
    "\n",
    "Train a classifier using the model that was trained using only the contrastive loss as base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "602e09f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T22:32:57.275634Z",
     "iopub.status.busy": "2022-08-07T22:32:57.274969Z",
     "iopub.status.idle": "2022-08-07T22:32:58.878850Z",
     "shell.execute_reply": "2022-08-07T22:32:58.877957Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(color_codes=True)\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForImageClassification, AutoConfig, AutoFeatureExtractor\n",
    "from transformers.utils import logging\n",
    "from datasets import Features, Value\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "logging.set_verbosity(transformers.logging.ERROR) \n",
    "logging.disable_progress_bar() \n",
    "\n",
    "p = os.path.abspath('../')\n",
    "sys.path.insert(1, p)\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from src.contrastive_transformers.losses import SupConLoss\n",
    "from src.utils.utils import *\n",
    "from src.transforms.transforms import Noise\n",
    "from torchvision.io import ImageReadMode\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "import torchvision\n",
    "from datasets import Image\n",
    "from src.utils.utils import *\n",
    "from src.wordnet_ontology.wordnet_ontology import WordnetOntology\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset \n",
    "\n",
    "seed=2401\n",
    "n_excluded_classes=int(556 * 0.05)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mapping_filename = './data/external/imagenet/LOC_synset_mapping.txt'\n",
    "wn = WordnetOntology(mapping_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578081a",
   "metadata": {},
   "source": [
    "## Preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "731eebf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Found cached dataset imagenet_sketch (/mnt/HDD/kevinds/sketch/./cache/imagenet_sketch/default/0.0.0/9bbda26372327ae1daa792112c8bbd2545a91b9f397ea6f285576add0a70ab6e)\n",
      "Loading cached processed dataset at /mnt/HDD/kevinds/sketch/./cache/imagenet_sketch/default/0.0.0/9bbda26372327ae1daa792112c8bbd2545a91b9f397ea6f285576add0a70ab6e/cache-c5da55713e638949.arrow\n"
     ]
    }
   ],
   "source": [
    "sketch = load_dataset(\"imagenet_sketch\", split='train', cache_dir='./cache/')\n",
    "vocab = torch.load('./models/vocab.pt')\n",
    "NUM_CLASSES = len(vocab)\n",
    "\n",
    "sketch = sketch.map(lambda x: {\n",
    "    'label': vocab[wn.hypernym(wn.class_for_index[x['label']])],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40bd0eba",
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "imagenet_classes_folder = './data/external/imagenet/ILSVRC/Data/CLS-LOC/train'\n",
    "\n",
    "image_labels = [] \n",
    "image_paths = []\n",
    "\n",
    "N_IMAGENET_EXAMPLES = 32\n",
    "imagenet_classes = sorted(os.listdir(imagenet_classes_folder))\n",
    "for img_class in imagenet_classes:\n",
    "    all_imgs = os.listdir(f\"{imagenet_classes_folder}/{img_class}/\")\n",
    "    img_names = [random.choice(all_imgs) for _ in range(0, N_IMAGENET_EXAMPLES)]\n",
    "                              \n",
    "    image_paths.extend([f\"{imagenet_classes_folder}/{img_class}/{name}\" for name in img_names])\n",
    "    image_labels.extend([img_class] * len(img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ee7473e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82303c3230b445f09a82399ef334a6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37961 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0db61eee9684c71b64007b23e54b1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12928 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_classes = list(set(sketch['label']))\n",
    "excluded_classes = [random.choice(_classes) for i in range(n_excluded_classes)]\n",
    "sketch = sketch.cast_column('image', Image(decode=False))\n",
    "\n",
    "dt = train_test_split(sketch, excluded_labels=excluded_classes)\n",
    "train_sketch, test_sketch = dt['train'], dt['test']\n",
    "def get_image_path(row):\n",
    "    return {'path': row['image']['path']}\n",
    "\n",
    "train = train_sketch.map(get_image_path, remove_columns=['image'])\n",
    "test = test_sketch.map(get_image_path, remove_columns=['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b783ae0",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /mnt/HDD/kevinds/sketch/./cache/imagenet_sketch/default/0.0.0/9bbda26372327ae1daa792112c8bbd2545a91b9f397ea6f285576add0a70ab6e/cache-1935775775b71dbc.arrow\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.concat([\n",
    "    pd.DataFrame({'image': train['path'], 'label': train['label']}), \n",
    "    pd.DataFrame({'image': image_paths, 'label': (vocab[wn.hypernym(lb)] for lb in image_labels)})\n",
    "], axis=0).reset_index(drop=True)\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = test_sketch.map(get_image_path, remove_columns=['image']).rename_column('path', 'image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c70bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "_transforms = torch.nn.Sequential(\n",
    "    transforms.Resize((feature_extractor.size, feature_extractor.size)),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
    ")\n",
    "\n",
    "def get_pixel_values(examples):\n",
    "    paths = examples['image']\n",
    "    images = [ _transforms(torchvision.io.read_image(path, ImageReadMode.RGB)) for path in paths]\n",
    "    examples[\"pixel_values\"] = images\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "train_dataset = train_dataset.with_transform(get_pixel_values)\n",
    "test_dataset = test_dataset.with_transform(get_pixel_values)\n",
    "collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009e395",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8f67ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyk = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    k=5\n",
    "    top1_pred = np.argmax(logits, axis=-1, keepdims=True)\n",
    "    top5_pred = np.argpartition(logits, -k, axis=-1)[:, -k:]\n",
    "    \n",
    "    top1 = accuracyk.compute(predictions=top1_pred, references=labels)\n",
    "    top5 = accuracyk.compute(predictions=top5_pred, references=labels)\n",
    "    return {\n",
    "        'top1': top1['accuracy'],\n",
    "        'top5': top5['accuracy']\n",
    "    }\n",
    "\n",
    "cb = StoreLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6580928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=556, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hub.set_dir('./cache')\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    f\"./models/contrastive-pretraining-{seed}/last-checkpoint\", \n",
    "    cache_dir='./cache/',\n",
    "    num_labels=len(vocab),\n",
    "    label2id=vocab.get_stoi(),\n",
    "    id2label=dict(enumerate(vocab.get_itos()))\n",
    ")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910235a5",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 69961\n",
      "  Num Epochs = 16\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 8736\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8083' max='8736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8083/8736 1:36:02 < 07:45, 1.40 it/s, Epoch 14.80/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.379400</td>\n",
       "      <td>2.184873</td>\n",
       "      <td>0.577661</td>\n",
       "      <td>0.868270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.958500</td>\n",
       "      <td>1.226911</td>\n",
       "      <td>0.699876</td>\n",
       "      <td>0.916306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.569200</td>\n",
       "      <td>1.108506</td>\n",
       "      <td>0.729425</td>\n",
       "      <td>0.921643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.303500</td>\n",
       "      <td>1.078849</td>\n",
       "      <td>0.747215</td>\n",
       "      <td>0.924660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>1.123052</td>\n",
       "      <td>0.750928</td>\n",
       "      <td>0.918007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>1.195078</td>\n",
       "      <td>0.751315</td>\n",
       "      <td>0.914449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.099700</td>\n",
       "      <td>1.192004</td>\n",
       "      <td>0.754796</td>\n",
       "      <td>0.921566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>1.254992</td>\n",
       "      <td>0.753868</td>\n",
       "      <td>0.916692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>1.199379</td>\n",
       "      <td>0.765393</td>\n",
       "      <td>0.920328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>1.262574</td>\n",
       "      <td>0.766244</td>\n",
       "      <td>0.916615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>1.259039</td>\n",
       "      <td>0.769415</td>\n",
       "      <td>0.921101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>1.225365</td>\n",
       "      <td>0.777073</td>\n",
       "      <td>0.924969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>1.243941</td>\n",
       "      <td>0.780476</td>\n",
       "      <td>0.922262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>1.230453</td>\n",
       "      <td>0.784267</td>\n",
       "      <td>0.923113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-546\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-546/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-546/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-546/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-1092\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-1092/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-1092/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-1092/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-1638\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-1638/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-1638/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-1638/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-546] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-2184\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-2184/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-2184/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-2184/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-1092] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-2730\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-2730/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-2730/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-2730/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-1638] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-3276\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-3276/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-3276/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-3276/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-2730] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-3822\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-3822/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-3822/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-3822/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-3276] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-4368\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-4368/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-4368/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-4368/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-3822] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-4914\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-4914/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-4914/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-4914/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-4368] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-5460\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-5460/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-5460/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-5460/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-4914] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-6006\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-6006/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-6006/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-6006/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-5460] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-6552\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-6552/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-6552/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-6552/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-6006] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-7098\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-7098/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-7098/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-7098/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-6552] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12928\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./models/classifier-training-2401/checkpoint-7644\n",
      "Configuration saved in ./models/classifier-training-2401/checkpoint-7644/config.json\n",
      "Model weights saved in ./models/classifier-training-2401/checkpoint-7644/pytorch_model.bin\n",
      "Feature extractor saved in ./models/classifier-training-2401/checkpoint-7644/preprocessor_config.json\n",
      "Deleting older checkpoint [models/classifier-training-2401/checkpoint-7098] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./models/classifier-training-{seed}/\",\n",
    "    resume_from_checkpoint=True,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=16,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=250,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=250,\n",
    "    logging_steps=50,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    fp16=True,\n",
    "    fp16_opt_level='03',\n",
    "    report_to=\"wandb\",\n",
    "    optim=\"adamw_torch\"\n",
    ")\n",
    "           \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    callbacks=[cb]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b51bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "ax = plt.GridSpec(1, 3, figure=fig)\n",
    "\n",
    "class_ax = plt.subplot(ax[0, 0])\n",
    "contr_ax = plt.subplot(ax[0, 1])\n",
    "test_ax = plt.subplot(ax[0, 2])\n",
    "\n",
    "class_ax.set_title(\"train loss\")\n",
    "contr_ax.set_title(\"eval loss\")\n",
    "test_ax.set_title(\"eval accuracy\") #classification loss on the test dataset\n",
    "\n",
    "sns.lineplot(x=range(1, len(cb.train_loss) + 1), y=cb.train_loss, ax=contr_ax)\n",
    "sns.lineplot(x=range(1, len(cb.eval_loss) + 1), y=cb.eval_loss, ax=class_ax)\n",
    "sns.lineplot(x=range(1, len(cb.top1) + 1), y=cb.top1, ax=test_ax)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

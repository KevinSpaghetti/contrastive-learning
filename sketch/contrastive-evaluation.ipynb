{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9778a8",
   "metadata": {},
   "source": [
    "## Evaluate models based on nearest-neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "602e09f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T22:32:57.275634Z",
     "iopub.status.busy": "2022-08-07T22:32:57.274969Z",
     "iopub.status.idle": "2022-08-07T22:32:58.878850Z",
     "shell.execute_reply": "2022-08-07T22:32:58.877957Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(color_codes=True)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForImageClassification, AutoConfig, AutoFeatureExtractor\n",
    "from transformers.utils import logging\n",
    "from transformers import DefaultDataCollator\n",
    "import scipy.spatial.distance as distance\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "logging.set_verbosity(transformers.logging.ERROR) \n",
    "logging.disable_progress_bar() \n",
    "\n",
    "p = os.path.abspath('../')\n",
    "sys.path.insert(1, p)\n",
    "\n",
    "from functools import partial \n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "import evaluate\n",
    "from src.utils.utils import *\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from datasets import Image, Features, Value\n",
    "\n",
    "from src.wordnet_ontology.wordnet_ontology import WordnetOntology\n",
    "from src.contrastive_transformers.collators import ImageCollator\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset \n",
    "\n",
    "seed=7631\n",
    "n_excluded_classes = int(556 * 0.05)\n",
    "N_EXAMPLES = 32\n",
    "batch_size = 16\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578081a",
   "metadata": {},
   "source": [
    "## Preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff3e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagenet_sketch (/mnt/HDD/kevinds/sketch/./cache/imagenet_sketch/default/0.0.0/9bbda26372327ae1daa792112c8bbd2545a91b9f397ea6f285576add0a70ab6e)\n",
      "Loading cached processed dataset at /mnt/HDD/kevinds/sketch/./cache/imagenet_sketch/default/0.0.0/9bbda26372327ae1daa792112c8bbd2545a91b9f397ea6f285576add0a70ab6e/cache-f827821a6a95155f.arrow\n"
     ]
    }
   ],
   "source": [
    "mapping_filename = './data/external/imagenet/LOC_synset_mapping.txt'\n",
    "wn = WordnetOntology(mapping_filename)\n",
    "\n",
    "vocab = torch.load('./models/vocab.pt')\n",
    "\n",
    "sketch = load_dataset(\"imagenet_sketch\", split='train', cache_dir='./cache/')\n",
    "sketch = sketch.map(lambda x: {\n",
    "    'label': vocab[wn.hypernym(wn.class_for_index[x['label']])],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40bd0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_classes_folder = './data/external/imagenet/ILSVRC/Data/CLS-LOC/train'\n",
    "\n",
    "image_labels = [] \n",
    "image_paths = []\n",
    "\n",
    "N_IMAGENET_EXAMPLES = N_EXAMPLES\n",
    "imagenet_classes = sorted(os.listdir(imagenet_classes_folder))\n",
    "for img_class in imagenet_classes:\n",
    "    all_imgs = os.listdir(f\"{imagenet_classes_folder}/{img_class}/\")\n",
    "    img_names = [random.choice(all_imgs) for _ in range(0, N_IMAGENET_EXAMPLES)]\n",
    "                              \n",
    "    image_paths.extend([f\"{imagenet_classes_folder}/{img_class}/{name}\" for name in img_names])\n",
    "    image_labels.extend([img_class] * len(img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79b6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "_classes = list(set(sketch['label']))\n",
    "excluded_classes = [random.choice(_classes) for i in range(n_excluded_classes)]\n",
    "dt = train_test_split(sketch, excluded_labels=excluded_classes)\n",
    "train, test = dt['train'], dt['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009e395",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6580928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hub.set_dir('./cache')\n",
    "model_path = f'./models/contrastive-pretraining-{seed}/last-checkpoint'\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b707947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc7518c27f94de6bb917afd956e50b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7647851ef47a4adf8d9c29f584fd2d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_transforms = Compose([\n",
    "    transforms.Resize((feature_extractor.size, feature_extractor.size)), \n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
    "])\n",
    "\n",
    "def get_pixel_values(example):\n",
    "    return test_transforms(example.convert(\"RGB\"))\n",
    "\n",
    "image_classes = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        'image': [img['path'] for img in train.cast_column('image', Image(decode=False))['image']], \n",
    "        'label': train['label']\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        'image': image_paths, \n",
    "        'label': [vocab[wn.hypernym(lb)] for lb in image_labels]\n",
    "})], axis=0)\n",
    "\n",
    "cl = image_classes.groupby('label').sample(n=N_EXAMPLES, replace=True)\n",
    "#cl = image_classes#.drop_duplicates(['label']).reset_index(drop=True)\n",
    "classes_dataset = Dataset.from_pandas(cl.reset_index(drop=True), features=Features({'image': Image(decode=True), 'label': Value('int64')}))\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "accuracyk = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "\n",
    "seen, unseen = get_seen_unseen_split(train, test, label_col='label')\n",
    "results= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32309a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_classes(model, classes, num_classes, label_col='label'):\n",
    "    class_encodings = torch.zeros((num_classes, model.config.hidden_size), device=device)\n",
    "    labels, counts = np.unique(classes[label_col], return_counts=True)\n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(0, len(classes), batch_size), total=len(classes) // batch_size):\n",
    "            batch_start = idx\n",
    "            batch_end = min(len(classes), batch_start+batch_size)\n",
    "            imgs = torch.stack([\n",
    "                test_transforms(cl.convert(\"RGB\")) for cl in classes[batch_start: batch_end]['image']\n",
    "            ])\n",
    "            model_output = model(pixel_values=imgs.to(device, non_blocking=True), output_hidden_states=True)\n",
    "            embedding = model_output.pooler_output\n",
    "            class_encodings[classes[batch_start:batch_end]['label']] += embedding\n",
    "    \n",
    "    class_encodings = class_encodings.to('cpu', non_blocking=True)\n",
    "    class_encodings /= counts[:, None]\n",
    "    return class_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c11c819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_samples(model, samples):\n",
    "    with torch.no_grad():\n",
    "        sample_encodings = torch.zeros((len(samples), model.config.hidden_size), device=device)\n",
    "        for idx in tqdm(range(0, len(samples), batch_size), total=len(samples) // batch_size):\n",
    "            batch_start = idx\n",
    "            batch_end = min(len(samples), batch_start+batch_size)\n",
    "            imgs = torch.stack([\n",
    "                test_transforms(sample.convert(\"RGB\")) for sample in samples[batch_start: batch_end]['image']\n",
    "            ])\n",
    "            model_output = model(pixel_values=imgs.to(device, non_blocking=True), output_hidden_states=True)\n",
    "            model_prediction = model_output.pooler_output\n",
    "            sample_encodings[batch_start:batch_end] = model_prediction\n",
    "    return sample_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbdfbd97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 4448/4448 [07:00<00:00, 10.57it/s]\n"
     ]
    }
   ],
   "source": [
    "class_encodings = encode_classes(model, classes_dataset, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c23bf0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3065it [01:59, 25.55it/s]                                                           \n",
      "2545it [01:39, 25.58it/s]                                                           \n",
      "100%|█████████████████████████████████████████████| 520/520 [00:21<00:00, 24.66it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_index = NearestNeighbors(n_neighbors=5, \n",
    "                                   metric='cosine', \n",
    "                                   algorithm='brute', \n",
    "                                   n_jobs=-1).fit(class_encodings.to('cpu'))\n",
    "\n",
    "encoded_samples = encode_samples(model, test)\n",
    "_, predictions = embedding_index.kneighbors(encoded_samples.cpu())\n",
    "results['complete'] = {\n",
    "    'top1': accuracyk.compute(predictions=predictions[:, 0][:, None], references=test['label'])['accuracy'],\n",
    "    'top5': accuracyk.compute(predictions=predictions, references=test['label'])['accuracy']\n",
    "}\n",
    "\n",
    "encoded_samples = encode_samples(model, seen)\n",
    "_, predictions = embedding_index.kneighbors(encoded_samples.cpu())\n",
    "results['seen'] = {\n",
    "    'top1': accuracyk.compute(predictions=predictions[:, 0][:, None], references=seen['label'])['accuracy'],\n",
    "    'top5': accuracyk.compute(predictions=predictions, references=seen['label'])['accuracy']\n",
    "}\n",
    "\n",
    "encoded_samples = encode_samples(model, unseen)\n",
    "_, predictions = embedding_index.kneighbors(encoded_samples.cpu())\n",
    "results['unseen'] = {\n",
    "    'top1': accuracyk.compute(predictions=predictions[:, 0][:, None], references=unseen['label'])['accuracy'],\n",
    "    'top5': accuracyk.compute(predictions=predictions, references=unseen['label'])['accuracy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "031a187a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'complete': {'top1': 0.7157542628701966, 'top5': 0.9423186750428326},\n",
       " 'seen': {'top1': 0.723887196619829, 'top5': 0.9443843961874816},\n",
       " 'unseen': {'top1': 0.6759615384615385, 'top5': 0.9322115384615385}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

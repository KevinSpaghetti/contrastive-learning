{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390f5816",
   "metadata": {},
   "source": [
    "## Evaluate models based on classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c24abe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T22:32:57.275634Z",
     "iopub.status.busy": "2022-08-07T22:32:57.274969Z",
     "iopub.status.idle": "2022-08-07T22:32:58.878850Z",
     "shell.execute_reply": "2022-08-07T22:32:58.877957Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(color_codes=True)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForImageClassification, AutoConfig, AutoFeatureExtractor\n",
    "from transformers.utils import logging\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "logging.set_verbosity(transformers.logging.ERROR) \n",
    "logging.disable_progress_bar() \n",
    "\n",
    "p = os.path.abspath('../')\n",
    "sys.path.insert(1, p)\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "import evaluate\n",
    "from src.utils.utils import *\n",
    "from src.wordnet_ontology.wordnet_ontology import WordnetOntology\n",
    "\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "import torchvision\n",
    "from datasets import Image\n",
    "from datasets import load_dataset \n",
    "from evaluate import evaluator\n",
    "\n",
    "seed=7631\n",
    "n_excluded_classes = int(556 * 0.05)\n",
    "N_EXAMPLES = 32\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c69937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagenet_sketch (/mnt/HDD/kevinds/sketch/./cache/imagenet_sketch/default/0.0.0/9bbda26372327ae1daa792112c8bbd2545a91b9f397ea6f285576add0a70ab6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af43e309e5b43e4bda85c3484f9a338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50889 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapping_filename = './data/external/imagenet/LOC_synset_mapping.txt'\n",
    "wn = WordnetOntology(mapping_filename)\n",
    "\n",
    "sketch = load_dataset(\"imagenet_sketch\", split='train', cache_dir='./cache/')\n",
    "vocab = torch.load('./models/vocab.pt')\n",
    "NUM_CLASSES = len(vocab)\n",
    "\n",
    "sketch = sketch.map(lambda x: {\n",
    "    'label': vocab[wn.hypernym(wn.class_for_index[x['label']])],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe5b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_classes = list(set(sketch['label']))\n",
    "excluded_classes = [random.choice(_classes) for i in range(n_excluded_classes)]\n",
    "dt = train_test_split(sketch, excluded_labels=excluded_classes)\n",
    "train, test = dt['train'], dt['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a8ee4f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3570f6f524a847e38cbca812c1624b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348e8f6e483546ac9954049e47d6f0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.hub.set_dir('../cache')\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    f\"./models/contrastive-classifier-{seed}/last-checkpoint\", \n",
    "    num_labels=NUM_CLASSES,\n",
    "    label2id=vocab.get_stoi(),\n",
    "    id2label=dict(enumerate(vocab.get_itos()))\n",
    ")\n",
    "\n",
    "test_transforms = Compose([\n",
    "    transforms.Resize((feature_extractor.size, feature_extractor.size)), \n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
    "])\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "seen, unseen = get_seen_unseen_split(train, test, label_col='label')\n",
    "score = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ed391",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_1 = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "accuracy_5 = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "\n",
    "for row in tqdm(test):\n",
    "    model_inputs = test_transforms(row['image'].convert('RGB')).to(device)\n",
    "    model_predictions = model(pixel_values=model_inputs[None, ...])\n",
    "    logits = model_predictions.get('logits').detach().cpu()\n",
    "    top1_pred = np.argmax(logits, axis=-1, keepdims=True)\n",
    "    top5_pred = np.argpartition(logits, -5, axis=-1)[:, -5:]\n",
    "    accuracy_1.add_batch(predictions=top1_pred, references=[row['label']])\n",
    "    accuracy_5.add_batch(predictions=top5_pred, references=[row['label']])\n",
    "score['complete']={\n",
    "    'top1': accuracy_1.compute()['accuracy'],\n",
    "    'top5': accuracy_5.compute()['accuracy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f7312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10177/10177 [03:20<00:00, 50.70it/s]\n"
     ]
    }
   ],
   "source": [
    "accuracy_1 = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "accuracy_5 = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "\n",
    "for row in tqdm(seen):\n",
    "    model_inputs = test_transforms(row['image'].convert('RGB')).to(device)\n",
    "    model_predictions = model(pixel_values=model_inputs[None, ...])\n",
    "    logits = model_predictions.get('logits').detach().cpu()\n",
    "    top1_pred = np.argmax(logits, axis=-1, keepdims=True)\n",
    "    top5_pred = np.argpartition(logits, -5, axis=-1)[:, -5:]\n",
    "    accuracy_1.add_batch(predictions=top1_pred, references=[row['label']])\n",
    "    accuracy_5.add_batch(predictions=top5_pred, references=[row['label']])\n",
    "score['seen']={\n",
    "    'top1': accuracy_1.compute()['accuracy'],\n",
    "    'top5': accuracy_5.compute()['accuracy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3ae19ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2897/2897 [01:00<00:00, 48.09it/s]\n"
     ]
    }
   ],
   "source": [
    "accuracy_1 = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "accuracy_5 = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "\n",
    "for row in tqdm(unseen):\n",
    "    model_inputs = test_transforms(row['image'].convert('RGB')).to(device)\n",
    "    model_predictions = model(pixel_values=model_inputs[None, ...])\n",
    "    logits = model_predictions.get('logits').detach().cpu()\n",
    "    top1_pred = np.argmax(logits, axis=-1, keepdims=True)\n",
    "    top5_pred = np.argpartition(logits, -5, axis=-1)[:, -5:]\n",
    "    accuracy_1.add_batch(predictions=top1_pred, references=[row['label']])\n",
    "    accuracy_5.add_batch(predictions=top5_pred, references=[row['label']])\n",
    "score['unseen']={\n",
    "    'top1': accuracy_1.compute()['accuracy'],\n",
    "    'top5': accuracy_5.compute()['accuracy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f77819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'complete': {'top1': 0.5797766559583907, 'top5': 0.7540156034878385},\n",
       " 'seen': {'top1': 0.6753463692640267, 'top5': 0.8358062297337133},\n",
       " 'unseen': {'top1': 0.24404556437694166, 'top5': 0.46668967897825336}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

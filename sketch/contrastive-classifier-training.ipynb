{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d998328",
   "metadata": {},
   "source": [
    "## Train the model using contrastive and classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "602e09f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T22:32:57.275634Z",
     "iopub.status.busy": "2022-08-07T22:32:57.274969Z",
     "iopub.status.idle": "2022-08-07T22:32:58.878850Z",
     "shell.execute_reply": "2022-08-07T22:32:58.877957Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(color_codes=True)\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForImageClassification, AutoTokenizer, AutoConfig, AutoFeatureExtractor\n",
    "from transformers.utils import logging\n",
    "from transformers import DefaultDataCollator\n",
    "import scipy.spatial.distance as distance\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "logging.set_verbosity(transformers.logging.ERROR) \n",
    "logging.disable_progress_bar() \n",
    "\n",
    "p = os.path.abspath('../')\n",
    "sys.path.insert(1, p)\n",
    "\n",
    "from torch.utils.data import DataLoader \n",
    "from functools import partial \n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from src.Loss.ContrastiveLoss import SupConLoss\n",
    "from src.utils.utils import *\n",
    "from src.transforms.transforms import Noise\n",
    "from src.wordnet_ontology.wordnet_ontology import WordnetOntology\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from datasets import Image\n",
    "\n",
    "from src.contrastive_transformers.datasets import AutoAugmentDataset\n",
    "from src.contrastive_transformers.trainers import ContrastiveTrainer\n",
    "from torchvision.transforms import CenterCrop\n",
    "from src.contrastive_transformers.collators import ImageCollator\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "n_excluded_classes = int(556 * 0.05)\n",
    "\n",
    "seed=7361\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mapping_filename = './data/external/imagenet/LOC_synset_mapping.txt'\n",
    "wn = WordnetOntology(mapping_filename)\n",
    "\n",
    "vocab = torch.load('./models/vocab.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578081a",
   "metadata": {},
   "source": [
    "## Preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fecb18f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagenet_sketch (/mnt/HDD/kevinds/sketch/./cache/imagenet_sketch/default/0.0.0/9bbda26372327ae1daa792112c8bbd2545a91b9f397ea6f285576add0a70ab6e)\n",
      "Loading cached processed dataset at /mnt/HDD/kevinds/sketch/./cache/imagenet_sketch/default/0.0.0/9bbda26372327ae1daa792112c8bbd2545a91b9f397ea6f285576add0a70ab6e/cache-5250d933f01067b4.arrow\n"
     ]
    }
   ],
   "source": [
    "sketch = load_dataset(\"imagenet_sketch\", split='train', cache_dir='./cache/')\n",
    "def get_hclass(x):\n",
    "    _class = wn.class_for_index[x['label']] \n",
    "    return { \n",
    "        'label': vocab[wn.hypernym(_class)] \n",
    "    }\n",
    "\n",
    "sketch = sketch.map(get_hclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acb5c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_classes = list(set(sketch['label']))\n",
    "excluded_classes = [random.choice(_classes) for i in range(n_excluded_classes)]\n",
    "dt = train_test_split(sketch, excluded_labels=excluded_classes)\n",
    "train, test = dt['train'], dt['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ea3b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n02085374',\n",
       " 'n04490091',\n",
       " 'n01632047',\n",
       " 'n12056217',\n",
       " 'n02329401',\n",
       " 'n04015204',\n",
       " 'n04100174',\n",
       " 'n02087122',\n",
       " 'n02453611',\n",
       " 'n02796623',\n",
       " 'n03211117',\n",
       " 'n03706653',\n",
       " 'n04202417',\n",
       " 'n02887209',\n",
       " 'n04549122',\n",
       " 'n01703569',\n",
       " 'n04586421',\n",
       " 'n12157056',\n",
       " 'n03953416',\n",
       " 'n04520170',\n",
       " 'n01816887',\n",
       " 'n02534734',\n",
       " 'n04187061',\n",
       " 'n06595351',\n",
       " 'n04151581',\n",
       " 'n02268148',\n",
       " 'n15074962']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab.get_itos()[_cl] for _cl in excluded_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40bd0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_classes_folder = './data/external/imagenet/ILSVRC/Data/CLS-LOC/train'\n",
    "\n",
    "image_labels = [] \n",
    "image_paths = []\n",
    "\n",
    "N_IMAGENET_EXAMPLES = 100\n",
    "imagenet_classes = sorted(os.listdir(imagenet_classes_folder))\n",
    "for img_class in imagenet_classes:\n",
    "    all_imgs = os.listdir(f\"{imagenet_classes_folder}/{img_class}/\")\n",
    "    img_names = [random.choice(all_imgs) for _ in range(0, N_IMAGENET_EXAMPLES)]\n",
    "                              \n",
    "    image_paths.extend([f\"{imagenet_classes_folder}/{img_class}/{name}\" for name in img_names])\n",
    "    image_labels.extend([img_class] * len(img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b783ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image and class data\n",
      "                                                    image  label\n",
      "0       ./cache/downloads/extracted/b7724a58d90ea10a91...     34\n",
      "1       ./cache/downloads/extracted/b7724a58d90ea10a91...    374\n",
      "2       ./cache/downloads/extracted/b7724a58d90ea10a91...    386\n",
      "3       ./cache/downloads/extracted/b7724a58d90ea10a91...    133\n",
      "4       ./cache/downloads/extracted/b7724a58d90ea10a91...    546\n",
      "...                                                   ...    ...\n",
      "138365  ./data/external/imagenet/ILSVRC/Data/CLS-LOC/t...    555\n",
      "138366  ./data/external/imagenet/ILSVRC/Data/CLS-LOC/t...    555\n",
      "138367  ./data/external/imagenet/ILSVRC/Data/CLS-LOC/t...    555\n",
      "138368  ./data/external/imagenet/ILSVRC/Data/CLS-LOC/t...    555\n",
      "138369  ./data/external/imagenet/ILSVRC/Data/CLS-LOC/t...    555\n",
      "\n",
      "[138370 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr = train.cast_column('image', Image(decode=False))\n",
    "train_data = pd.concat([\n",
    "    pd.DataFrame({'image': [p['path'] for p in tr['image']], 'label': tr['label']}), \n",
    "    pd.DataFrame({'image': image_paths, 'label': [vocab[wn.hypernym(cl)] for cl in image_labels]})\n",
    "], axis=0).reset_index(drop=True)\n",
    "print(\"Image and class data\")\n",
    "print(train_data)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "effd4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "augment = transforms.Compose([\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.RandomApply([Noise(0.25)], p=0.5),\n",
    "    CenterCrop(feature_extractor.size),  \n",
    "    Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
    "])\n",
    "transform = Compose([\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    CenterCrop(feature_extractor.size),  \n",
    "    Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
    "])\n",
    "\n",
    "# final huggingface dataset\n",
    "train = AutoAugmentDataset(train_data['image'], train_data['label'], return_negative=False)\n",
    "data_collator = ImageCollator(transform, None, augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009e395",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6580928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.set_dir('../cache')\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\", \n",
    "    num_labels=len(vocab),\n",
    "    label2id=vocab.get_stoi(),\n",
    "    id2label=dict(enumerate(vocab.get_itos()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f67ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyk = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    k=5\n",
    "    top1_pred = np.argmax(logits, axis=-1, keepdims=True)\n",
    "    top5_pred = np.argpartition(logits, -k, axis=-1)[:, -k:]\n",
    "    \n",
    "    top1 = accuracyk.compute(predictions=top1_pred, references=labels)\n",
    "    top5 = accuracyk.compute(predictions=top5_pred, references=labels)\n",
    "    return {\n",
    "        'top1': top1['accuracy'],\n",
    "        'top5': top5['accuracy']\n",
    "    }\n",
    "\n",
    "cb = StoreLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910235a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 138370\n",
      "  Num Epochs = 16\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 8640\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevindellaschiava\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/HDD/kevinds/sketch/wandb/run-20221026_211933-yy5xdadh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kevindellaschiava/huggingface/runs/yy5xdadh\" target=\"_blank\">./models/contrastive-classifier-4896</a></strong> to <a href=\"https://wandb.ai/kevindellaschiava/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8640' max='8640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8640/8640 6:53:57, Epoch 15/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>127.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>70.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>52.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>41.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>35.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>30.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>27.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>25.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>23.898700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>22.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>21.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>20.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>19.463600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>18.752900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>18.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>17.952700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>17.625700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-540\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-540/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-540/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-540/preprocessor_config.json\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-1080\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-1080/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-1080/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-1080/preprocessor_config.json\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-1620\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-1620/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-1620/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-1620/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-540] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-2160\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-2160/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-2160/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-2160/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-1080] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-2700\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-2700/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-2700/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-2700/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-1620] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-3240\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-3240/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-3240/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-3240/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-2160] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-3780\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-3780/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-3780/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-3780/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-2700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-4320\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-4320/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-4320/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-4320/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-3240] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-4860\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-4860/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-4860/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-4860/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-3780] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-5400\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-5400/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-5400/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-5400/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-4320] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-5940\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-5940/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-5940/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-5940/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-4860] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-6480\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-6480/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-6480/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-6480/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-5400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-7020\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-7020/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-7020/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-7020/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-5940] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-7560\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-7560/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-7560/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-7560/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-6480] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-8100\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-8100/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-8100/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-8100/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-7020] due to args.save_total_limit\n",
      "Saving model checkpoint to ./models/contrastive-classifier-4896/checkpoint-8640\n",
      "Configuration saved in ./models/contrastive-classifier-4896/checkpoint-8640/config.json\n",
      "Model weights saved in ./models/contrastive-classifier-4896/checkpoint-8640/pytorch_model.bin\n",
      "Feature extractor saved in ./models/contrastive-classifier-4896/checkpoint-8640/preprocessor_config.json\n",
      "Deleting older checkpoint [models/contrastive-classifier-4896/checkpoint-7560] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8640, training_loss=34.508338165283206, metrics={'train_runtime': 24844.9731, 'train_samples_per_second': 89.109, 'train_steps_per_second': 0.348, 'total_flos': 0.0, 'train_loss': 34.508338165283206, 'epoch': 16.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./models/contrastive-classifier-{seed}\",\n",
    "    num_train_epochs=16,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy='epoch',\n",
    "    disable_tqdm=False,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=500,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    fp16=True,\n",
    "    fp16_opt_level='03',\n",
    "    report_to=\"wandb\",\n",
    "    optim=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "contrastive_head = nn.Sequential(\n",
    "    nn.Linear(768, 768 // 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(768 // 2, 768 // 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(768 // 4, 768 // 8),\n",
    ")\n",
    "\n",
    "cl_loss = nn.CrossEntropyLoss()\n",
    "ct_loss = SupConLoss(0.2)\n",
    "\n",
    "def loss_adapter(anchor_encodings, \n",
    "                 positive_encodings, \n",
    "                 negative_encodings, \n",
    "                 labels, \n",
    "                 negative_labels, \n",
    "                 anchor_outputs, positive_outputs, negative_outputs):\n",
    "    contrastive_loss = (\n",
    "        ct_loss(anchor_encodings, positive_encodings, labels) + \n",
    "        ct_loss(positive_encodings, anchor_encodings, labels)\n",
    "    )\n",
    "    classification_loss = (\n",
    "        cl_loss(anchor_outputs.get('logits'), labels) + \n",
    "        cl_loss(positive_outputs.get('logits'), labels)\n",
    "    )   \n",
    "    if negative_labels is not None and negative_outputs is not None:\n",
    "        classification_loss = cl_loss(negative_outputs.get('logits'), negative_labels)\n",
    "    \n",
    "    return classification_loss + contrastive_loss\n",
    "\n",
    "trainer = ContrastiveTrainer(\n",
    "    loss=loss_adapter,\n",
    "    head=contrastive_head,\n",
    "    use_negatives=False,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    callbacks=[cb]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5b51bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "ax = plt.GridSpec(1, 3, figure=fig)\n",
    "\n",
    "class_ax = plt.subplot(ax[0, 0])\n",
    "contr_ax = plt.subplot(ax[0, 1])\n",
    "test_ax = plt.subplot(ax[0, 2])\n",
    "\n",
    "class_ax.set_title(\"train loss\")\n",
    "contr_ax.set_title(\"eval loss\")\n",
    "test_ax.set_title(\"eval accuracy\") #classification loss on the test dataset\n",
    "\n",
    "sns.lineplot(x=range(1, len(cb.train_loss) + 1), y=cb.train_loss, ax=contr_ax)\n",
    "sns.lineplot(x=range(1, len(cb.eval_loss) + 1), y=cb.eval_loss, ax=class_ax)\n",
    "sns.lineplot(x=range(1, len(cb.top1) + 1), y=cb.top1, ax=test_ax)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
